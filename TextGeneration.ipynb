{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JElWJM4ty2ZJ",
        "SJUZwQphfd0r",
        "yyWzHoNFy9xm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JElWJM4ty2ZJ",
        "colab_type": "text"
      },
      "source": [
        "# Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kLIq9lleo7M",
        "colab_type": "code",
        "outputId": "dcd1dce7-976c-469f-d4bc-eaabb3a21751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Connecting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmGAOoSf_Sy",
        "colab_type": "code",
        "outputId": "51bc035d-645c-4dfa-eda7-adfbd41c070b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# importing all libraries\n",
        "\n",
        "import string\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from pickle import dump\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from random import randint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import floor\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJUZwQphfd0r",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAMkQEaTflSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Load doc into memory\n",
        "# Input: file name\n",
        "# Output: text\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpB5XaG4f4cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: turn a doc into clean tokens\n",
        "# Input: document\n",
        "# Output: a list of tokens that are all in lowercase, have no special characters, split at white spaces\n",
        "\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token ; string-to-replace, string-to-be-replaced-with, string-to-delete\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vngMwI6fhvKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: organize into sequences of tokens\n",
        "# Input: document tokens, input sequence length\n",
        "# Output: document broken down into sequences seperated by WS\n",
        "\n",
        "def create_sequences(tokens, seq_length):\n",
        "  print('Input sequence length is %d'% seq_length)\n",
        "  # length = in_length + out_length\n",
        "  total_length = seq_length+1\n",
        "  #declare a variable to hold the sequences\n",
        "  doc_sequences = list()\n",
        "  for i in range(total_length, len(tokens)):\n",
        "    # list of tokens; list size = total sequence length\n",
        "    curr_line_seq = tokens[i-total_length:i]\n",
        "    # create one string sequence seperated by WS\n",
        "    curr_line = ' '.join(curr_line_seq)\n",
        "    # append it to list of such sequences\n",
        "    doc_sequences.append(curr_line)\n",
        "  return doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM-lk__rjGyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: save a document\n",
        "# Input: document , filename\n",
        "# Output: none\n",
        "\n",
        "def save_doc(doc, doc_filename):\n",
        "\tdata = '\\n'.join(doc)\n",
        "\tfile = open(doc_filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqS4_TDjYwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: load a file containing document as sequence\n",
        "# Input: filename\n",
        "# Output: document sequences\n",
        "\n",
        "def load_sequenced_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\tsaved_doc_sequences = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn saved_doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjpPZTpkAEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: convert sequences to integer vectors\n",
        "# Input: list of sequences (text)\n",
        "# Output: list of sequences (int) (list of lists) , vocabulary size\n",
        "# Uses: save_tokenizer_artifacts\n",
        "\n",
        "def text_to_int_tokenize(text_seq, tokenizer_name):\n",
        "  # tokenize\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text_seq)\n",
        "  int_seq = tokenizer.texts_to_sequences(text_seq)\n",
        "\n",
        "  # find the vocab size\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  save_tokenizer_artifacts(tokenizer, tokenizer_name)\n",
        "  return int_seq, vocab_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzJcAw3glWt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Create input and labels\n",
        "# Input: tokenized document sequences (), size of vocabulary\n",
        "# Output: inputs (), Labels ()\n",
        "\n",
        "def create_inputs_and_labels(tokenized_doc_seq, vocab_size):\n",
        "  # convert list to array\n",
        "  array_tokenized_doc_seq = array(tokenized_doc_seq)\n",
        "  inputs = array_tokenized_doc_seq[:,:-1]\n",
        "  labels = array_tokenized_doc_seq[:,-1]\n",
        "  # one hot encode labels; # columns = vocab size\n",
        "  labels = to_categorical(labels, num_classes=vocab_size)\n",
        "  return inputs,labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VfIDGDnvwpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save model and tokenizer to file\n",
        "# Input: \n",
        "# Output: \n",
        "\n",
        "def save_tokenizer_artifacts(tokenizer, tokenizer_name):\n",
        "  \n",
        "  # save the tokenizer\n",
        "  dump(tokenizer, open(tokenizer_name, 'wb'))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URXAOFL_v9nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save model and tokenizer to file\n",
        "# Input: \n",
        "# Output: \n",
        "\n",
        "def save_model_artifacts(model_name, model):\n",
        "  # save the model to file\n",
        "  model.save(model_name)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLjjGsanoy-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: takes in the model, tokenizer, seed text, length of sequence and # words to be predicted\n",
        "# Input: \n",
        "# Output: concatenated predicted words\n",
        "\n",
        "def generate_seq(model, tokenizer, input_seq_length, in_text, n_words):\n",
        "\tresult = list()\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=input_seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM__vml9pPWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: \n",
        "# Input: name of the file containing the test sequences, name of the model and the tokenizer\n",
        "# Output: seed text and generated text\n",
        "# Uses: generate_seq()\n",
        "\n",
        "def predict_from_seed_data(in_filename, model_name, tokenizer_name):\n",
        "  \n",
        "  # load cleaned text sequences: list oflists\n",
        "  text_sequence_doc = load_doc(in_filename)\n",
        "  text_sequence_lines = text_sequence_doc.split('\\n')\n",
        "  # text_sequence_lines = ['it was my fault to think that the little boy was poor \n",
        "  #                       and unhappy I could not see the sad look the wicked woman \n",
        "  #                       used to give me as she handed me my bread unfortunately I was \n",
        "  #                       always hungry he emptied his glass while reading the newspaper \n",
        "  #                       he laid his napkin'] used this for testing\n",
        "  \n",
        "  # load the model\n",
        "  my_model = load_model(model_name)\n",
        "  \n",
        "  # load the tokenizer\n",
        "  my_tokenizer = load(open(tokenizer_name, 'rb'))\n",
        "  \n",
        "  seed_text = text_sequence_lines[randint(0,len(text_sequence_lines))]\n",
        "  print(seed_text + '\\n')\n",
        "  \n",
        "  input_seq_length = len(text_sequence_lines[0].split()) - 1\n",
        "  generated_text = generate_seq(my_model, my_tokenizer, input_seq_length, seed_text, 50)\n",
        "  \n",
        "  return seed_text, generated_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANW3StNxuTw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function\n",
        "\n",
        "\n",
        "def print_model(model, model_name):\n",
        "\n",
        "  plot_model(model, \n",
        "             show_shapes=True, \n",
        "             show_layer_names=True, \n",
        "             to_file = model_name)\n",
        "  \n",
        "  from google.colab import files\n",
        "  files.download(model_name)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBcWRVKnUrSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_acc_loss(model):\n",
        "  plt.plot(model.history['acc'])\n",
        "  plt.plot(model.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(model.history['loss'])\n",
        "  plt.plot(model.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llv8GmIFqMbh",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvj-FdtfqWMY",
        "colab_type": "text"
      },
      "source": [
        "**Model 1: Embedded - LSTM - Dense**\n",
        "\n",
        "Lets implement early stopping here since this model is quite slow even with smaller texts to begin with.\n",
        "\n",
        "Note: Might want to restore the defaults so that it can be used for bench marking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZhdMkN9qeiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector\n",
        "\n",
        "def Embed_LSTM_Dense(vocab_size, input_seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = vocab_size, \n",
        "                      output_dim = 50, \n",
        "                      input_length=input_seq_length))\n",
        "  model.add(LSTM(20, return_sequences=True))\n",
        "  model.add(LSTM(10))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLnM-VL0t0tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_model = Embed_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                               optimizer='adam', \n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_model_history = Embed_LSTM_Dense_model.fit(X_data, \n",
        "                                                            Y_data, \n",
        "                                                            batch_size=128, \n",
        "                                                            epochs=1) \n",
        "\n",
        "# print model\n",
        "print_model(Embed_LSTM_Dense_model, 'Embed_LSTM_Dense_model.png')\n",
        "\n",
        "# save model\n",
        "model_name = 'model_1.h5'\n",
        "\n",
        "save_model_artifacts(model_name, Embed_LSTM_Dense_model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pukk8frIsXbs",
        "colab_type": "text"
      },
      "source": [
        "**Model 2: Embedded - Conv - LSTM - Dense**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkMJ1WTMsfdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector, Drop out rate\n",
        "\n",
        "def Embed_Conv_LSTM_Dense(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                             output_dim = 100, \n",
        "                             input_length=input_seq_length))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(64, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    return model\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDNRpQP7yCwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "# Create\n",
        "Embed_Conv_LSTM_Dense_model = Embed_Conv_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_Conv_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_Conv_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                                    optimizer='adam', \n",
        "                                    metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "Embed_Conv_LSTM_Dense_model.fit(X_data, \n",
        "                                Y_data, \n",
        "                                batch_size=128, \n",
        "                                epochs=1) \n",
        "\n",
        "# print model\n",
        "print_model(Embed_Conv_LSTM_Dense_model, 'Embed_Conv_LSTM_Dense_model.png')\n",
        "\n",
        "# save model\n",
        "model_name = 'model_2.h5'\n",
        "\n",
        "save_model_artifacts(model_name, Embed_Conv_LSTM_Dense_model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37g_M9zkgK7",
        "colab_type": "text"
      },
      "source": [
        "**Model 3: Embed - LSTM - Dense with only 4 layers**\n",
        "\n",
        "Simple 4 layer model with input sequence = 0.5 of output dimentionality of embedding layer and output dimentionality of embedding layer = # units of LSTM layer that follows.\n",
        "\n",
        "Lets implement callbacks here since we know this model works well with the previous data set. So with a bigger text, we would like to save our model along the way so that we have something to work with of the session crashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV_JVoNXd4pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: None as of now since this model converges well and gives impressive results\n",
        "# number at the end indicates number of layers\n",
        "# Observations: # dims for embed layer = # LSTM units = # Dense units - kept on purpose\n",
        "# might consider experimenting later to see if performance is affected\n",
        "\n",
        "def Embed_LSTM_Dense_4(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                    output_dim = 100, \n",
        "                    input_length = input_seq_length))\n",
        "    # input: (samples = 2338, features = 50)\n",
        "    # output: (timesteps = 2338, samples = 50, features = 100)\n",
        "    # added dropout when it performed bad with validation data\n",
        "    model.add(LSTM(units = 100, \n",
        "                   dropout = 0.5,\n",
        "                   recurrent_dropout = 0.2,\n",
        "                   activation = 'tanh', \n",
        "                   use_bias = 'False'))\n",
        "    # output: (None, 100) \n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    # output: (None, 100) \n",
        "    # following layer should always have # units = dict size so that it matches the output\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # output: (None, 856) \n",
        "    # Shape of labels is: (2338, 856)\n",
        "    return model\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FTVrZaAfk7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_4_model = Embed_LSTM_Dense_4(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_4_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_4_model.compile(loss='categorical_crossentropy', \n",
        "                                 optimizer='adam', \n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "# adding a checkpoint| we monitor accuracy so the mode will be maximixed; for loss, its minimized\n",
        "# we can leave it as auto but I'm explicitely mentioning as max\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_savepoint = keras.callbacks.ModelCheckpoint(filepath='model_3_best_bread.h5', \n",
        "                                                  monitor='val_acc', \n",
        "                                                  verbose = 0 ,\n",
        "                                                  save_best_only=True ,\n",
        "                                                  save_weights_only=False, \n",
        "                                                  mode='max', \n",
        "                                                  period=1)\n",
        "\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_4_model_history = Embed_LSTM_Dense_4_model.fit(x = X_data, \n",
        "                                                                y = Y_data, \n",
        "                                                                callbacks = [model_savepoint],\n",
        "                                                                batch_size=2, \n",
        "                                                                epochs=40) \n",
        "\n",
        "# plot\n",
        "plot_acc_loss(Embed_LSTM_Dense_4_model_history)\n",
        "\n",
        "# print model\n",
        "#print_model(Embed_LSTM_Dense_4_model, 'Embed_LSTM_Dense_4_model.png')\n",
        "\n",
        "# save model\n",
        "# model name: model_3 is trainedon A piece of Bread\n",
        "# commenting out below as implementing callbacks instead\n",
        "# model_name = 'model_3_republic.h5'\n",
        "\n",
        "# save_model_artifacts(model_name, Embed_LSTM_Dense_4_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyWzHoNFy9xm",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9xJ5gEzD9F",
        "colab_type": "code",
        "outputId": "256f9f45-7e2d-4488-cd8d-817de96b3741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# load document\n",
        "# Variable doc is just a string: <class 'str'>\n",
        "\n",
        "in_filename = \"/content/gdrive/My Drive/DL/APieceOfBread.txt\"\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:200])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A PIECE OF BREAD\n",
            "\n",
            "BY FRANCOIS COPPEE\n",
            "\n",
            "\n",
            "The young Duc de Hardimont happened to be at Aix in Savoy, whose waters he\n",
            "hoped would benefit his famous mare, Perichole, who had become wind-broken\n",
            "since the c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkfBAE6mzi6e",
        "colab_type": "code",
        "outputId": "122e6362-e98b-4f7a-9e4c-166b40b57f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# clean document\n",
        "# Variable token is a list of strings(words): <class 'list'>\n",
        "\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'piece', 'of', 'bread', 'by', 'francois', 'coppee', 'the', 'young', 'duc', 'de', 'hardimont', 'happened', 'to', 'be', 'at', 'aix', 'in', 'savoy', 'whose', 'waters', 'he', 'hoped', 'would', 'benefit', 'his', 'famous', 'mare', 'perichole', 'who', 'had', 'become', 'windbroken', 'since', 'the', 'cold', 'she', 'had', 'caught', 'at', 'the', 'last', 'derby', 'and', 'was', 'finishing', 'his', 'breakfast', 'while', 'glancing', 'over', 'the', 'morning', 'paper', 'when', 'he', 'read', 'the', 'news', 'of', 'the', 'disastrous', 'engagement', 'at', 'reichshoffen', 'he', 'emptied', 'his', 'glass', 'of', 'chartreuse', 'laid', 'his', 'napkin', 'upon', 'the', 'restaurant', 'table', 'ordered', 'his', 'valet', 'to', 'pack', 'his', 'trunks', 'and', 'two', 'hours', 'later', 'took', 'the', 'express', 'to', 'paris', 'arriving', 'there', 'he', 'hastened', 'to', 'the', 'recruiting', 'office', 'and', 'enlisted', 'in', 'a', 'regiment', 'of', 'the', 'line', 'in', 'vain', 'had', 'he', 'led', 'the', 'enervating', 'life', 'of', 'a', 'fashionable', 'swell', 'that', 'was', 'the', 'word', 'of', 'the', 'time', 'and', 'had', 'knocked', 'about', 'racecourse', 'stables', 'from', 'the', 'age', 'of', 'nineteen', 'to', 'twentyfive', 'in', 'circumstances', 'like', 'these', 'he', 'could', 'not', 'forget', 'that', 'enguerrand', 'de', 'hardimont', 'died', 'of', 'the', 'plague', 'at', 'tunis', 'the', 'same', 'day', 'as', 'saint', 'louis', 'that', 'jean', 'de', 'hardimont', 'commanded', 'the', 'free', 'companies', 'under', 'du', 'guesclin', 'and', 'that', 'francoishenri', 'de', 'hardimont', 'was', 'killed', 'at', 'fontenoy', 'with', 'red', 'maison', 'upon', 'learning', 'that', 'france', 'had', 'lost', 'a', 'battle', 'on', 'french', 'soil']\n",
            "Total Tokens: 2389\n",
            "Unique Tokens: 855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKosSLKs0ArB",
        "colab_type": "code",
        "outputId": "6d0095ed-2f8d-4f8b-ab3d-d7207e53b4d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# create sequences from tokens\n",
        "# doc_to_sequences is a list of sequences of 50 words: <class 'list'>\n",
        "\n",
        "doc_to_sequences = list()\n",
        "sequence_length = 50\n",
        "\n",
        "doc_to_sequences = create_sequences(tokens,sequence_length)\n",
        "print('Total Sequences: %d' % len(doc_to_sequences))\n",
        "print(doc_to_sequences[:5])\n",
        "print('\\n'.join(doc_to_sequences[:5]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence length is 50\n",
            "Total Sequences: 2338\n",
            "['a piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over', 'piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the', 'of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning', 'bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper', 'by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper when']\n",
            "a piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over\n",
            "piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the\n",
            "of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning\n",
            "bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper\n",
            "by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper when\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggvvoyk70rsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save sequences to file to be used later for prediction\n",
        "\n",
        "out_filename = 'APieceOfBread_sequences.txt'\n",
        "save_doc(doc_to_sequences, out_filename)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBc04nkY1ctW",
        "colab_type": "code",
        "outputId": "c97c38d7-3e0f-4944-d4aa-59d9b2f87e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# load saved text sequences. do this step if you already have the sequences created earlier\n",
        "# doc_lines_seq is a list of sequences of 50 words: <class 'list'>, each elemnt within that list is a string\n",
        "# print(type(doc_lines_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_seq[0])) : <class 'str'>\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'\n",
        "doc_sequences = load_sequenced_doc(in_filename)\n",
        "# load it into a list for processing. Splitting into elemennts by newline\n",
        "doc_lines_seq = doc_sequences.split('\\n')\n",
        "print('Before tokenizing, sample length of input string sequence %d' % len(doc_lines_seq[0]))\n",
        "print(doc_lines_seq[:5])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before tokenizing, sample length of input string sequence 281\n",
            "['a piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over', 'piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the', 'of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning', 'bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper', 'by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper when']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O73W0Vo14Wf",
        "colab_type": "code",
        "outputId": "5df04d84-ac96-47d8-8448-aebdb7c02744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Vectorize the sequences using tokenizer. A word to int mapping dictionary will be created\n",
        "# doc_lines_int_seq is a list of lists. Each sequence is a list\n",
        "# print(type(doc_lines_int_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_int_seq[0])) : <class 'list'>\n",
        "\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "doc_lines_int_seq, sizeof_vocab = text_to_int_tokenize(doc_lines_seq, tokenizer_name)\n",
        "\n",
        "print('After tokenizing/vectorizing, sample length of input integers sequence %d' % len(doc_lines_int_seq[0]))\n",
        "print('Size of the vocabulary build by the tokenizer %d' % sizeof_vocab)\n",
        "\n",
        "print(doc_lines_int_seq[:5])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After tokenizing/vectorizing, sample length of input integers sequence 51\n",
            "Size of the vocabulary build by the tokenizer 856\n",
            "[[4, 115, 3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59], [115, 3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1], [3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156], [29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156, 270], [21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156, 270, 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUBjAShl3zT8",
        "colab_type": "code",
        "outputId": "152efdc3-11a6-41a3-b177-62481e3faea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create data\n",
        "# X_data is 2D tensor (numpy array since data type is same) ; Y_data is 2D tensor\n",
        "# X_data is <class 'numpy.ndarray'> with dimensions # samples, features = words in sequence\n",
        "# Y_data is <class 'numpy.ndarray'> with dimension # samples, features = vocab size - sparse array\n",
        "\n",
        "X_data, Y_data = create_inputs_and_labels(doc_lines_int_seq, sizeof_vocab)\n",
        "\n",
        "print(\"Shape of inputs is: {0}\".format(X_data.shape))\n",
        "print(\"Shape of labels is: {0}\".format(Y_data.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of inputs is: (2338, 50)\n",
            "Shape of labels is: (2338, 856)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de8SpD78XutV",
        "colab_type": "code",
        "outputId": "780ea374-aa75-494e-fc79-3a8fcf5d5573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(type(Y_data))\n",
        "print(type(Y_data[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKbEQUzYjsWX",
        "colab_type": "code",
        "outputId": "9d1d3c69-710a-47e7-961f-b03b33dc7253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Better to also have test, validation, and training sets for rigorous testing\n",
        "from math import floor\n",
        "\n",
        "ratio_train = 0.65\n",
        "ratio_valid = 0.35\n",
        "\n",
        "num_observations = len(X_data)\n",
        "\n",
        "upper_idx_test = floor(num_observations*ratio_train)\n",
        "upper_idx_valid = upper_idx_test + floor(num_observations*ratio_valid)\n",
        "\n",
        "# seperate out the test\n",
        "\n",
        "X_train = X_data[:upper_idx_test]\n",
        "Y_train = Y_data[:upper_idx_test]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[upper_idx_test:upper_idx_valid]\n",
        "Y_validation = Y_data[upper_idx_test:upper_idx_valid]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[upper_idx_valid:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "\n",
        "Y_test = next_in_sequence_actual_int[upper_idx_valid:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Input shape (1519, 50) ; Output shape (1519, 856)\n",
            "Validation Input shape (818, 50) ; Output shape (818, 856)\n",
            "Test Input shape (1, 50) ; Output shape (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVur8kDX1KFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compact version for experimenting with text: republic.txt\n",
        "\n",
        "in_filename = \"/content/gdrive/My Drive/republic.txt\"    # change this\n",
        "doc = load_doc(in_filename)\n",
        "\n",
        "tokens = clean_doc(doc)\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n",
        "doc_to_sequences = list()\n",
        "sequence_length = 50\n",
        "\n",
        "doc_to_sequences = create_sequences(tokens,sequence_length)\n",
        "print('Total Sequences: %d' % len(doc_to_sequences))\n",
        "\n",
        "out_filename = 'republic_sequences.txt'      # change this\n",
        "save_doc(doc_to_sequences, out_filename)\n",
        "\n",
        "in_filename = 'republic_sequences.txt'      # change this\n",
        "doc_sequences = load_sequenced_doc(in_filename)\n",
        "# load it into a list for processing. Splitting into elemennts by newline\n",
        "doc_lines_seq = doc_sequences.split('\\n')\n",
        "print('Before tokenizing, sample length of input string sequence %d' % len(doc_lines_seq[0]))\n",
        "\n",
        "tokenizer_name = 'tokenizer_republic.pkl'      # change this\n",
        "doc_lines_int_seq, sizeof_vocab = text_to_int_tokenize(doc_lines_seq, tokenizer_name)\n",
        "\n",
        "print('After tokenizing/vectorizing, sample length of input integers sequence %d' % len(doc_lines_int_seq[0]))\n",
        "print('Size of the vocabulary build by the tokenizer %d' % sizeof_vocab)\n",
        "\n",
        "X_data, Y_data = create_inputs_and_labels(doc_lines_int_seq, sizeof_vocab)\n",
        "\n",
        "print(\"Shape of inputs is: {0}\".format(X_data.shape))\n",
        "print(\"Shape of labels is: {0}\".format(Y_data.shape))\n",
        "\n",
        "\"\"\"\n",
        "# use this for dividing the datset into test, train, validation dataset\n",
        "ratio_train = 0.65\n",
        "ratio_valid = 0.35\n",
        "\n",
        "num_observations = len(X_data)\n",
        "\n",
        "upper_idx_test = floor(num_observations*ratio_train)\n",
        "upper_idx_valid = upper_idx_test + floor(num_observations*ratio_valid)\n",
        "\n",
        "# seperate out the test\n",
        "\n",
        "X_train = X_data[:upper_idx_test]\n",
        "Y_train = Y_data[:upper_idx_test]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[upper_idx_test:upper_idx_valid]\n",
        "Y_validation = Y_data[upper_idx_test:upper_idx_valid]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[upper_idx_valid:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "\n",
        "Y_test = next_in_sequence_actual_int[upper_idx_valid:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOZfVW5QVuUb",
        "colab_type": "text"
      },
      "source": [
        "# Predicting the next word - Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9N4_iCHV1jc",
        "colab_type": "code",
        "outputId": "1a5aa2ee-d70b-4fc1-c91e-98da6079673b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Model: Model 1\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_1.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the morning paper when he read the news of the disastrous engagement at reichshoffen he emptied his glass of chartreuse laid his napkin upon the restaurant table ordered his valet to pack his trunks and two hours later took the express to paris arriving there he hastened to the recruiting office\n",
            "\n",
            "the morning paper when he read the news of the disastrous engagement at reichshoffen he emptied his glass of chartreuse laid his napkin upon the restaurant table ordered his valet to pack his trunks and two hours later took the express to paris arriving there he hastened to the recruiting office\n",
            "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3d5KP9WGFb",
        "colab_type": "code",
        "outputId": "66333b5d-db2d-4e18-e0ed-b7a788a224ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Model: Model 2\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_2.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the bread was hard and had a bitter taste no fresh would be given until the next mornings distribution so the commissary officer had willed it this was certainly a very hard life sometimes the remembrance of former breakfasts came to him such as he had called hygienic when the day\n",
            "\n",
            "the bread was hard and had a bitter taste no fresh would be given until the next mornings distribution so the commissary officer had willed it this was certainly a very hard life sometimes the remembrance of former breakfasts came to him such as he had called hygienic when the day\n",
            "it it it it it it it it it it it it it it it it it it it the the the the the the the the the the the the the the the the the the the it it it the the it it it it it it it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EY7oQ3hQzk",
        "colab_type": "code",
        "outputId": "57dc9b36-a877-4418-d2a7-384a93a6e3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Model: Model 3\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_3_best_bread.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(\"With random seed data....\")\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n",
        "\n",
        "# with test data\n",
        "print(\"With test data....\")\n",
        "\n",
        "next_in_sequence_int = Embed_LSTM_Dense_4_model.predict_classes(X_test)\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "next_in_sequence_predicted = list()\n",
        "\n",
        "for word_int in next_in_sequence_int:\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_1 = word\n",
        "      break\n",
        "  next_in_sequence_predicted.append(out_word_1)\n",
        "\n",
        "next_in_sequence_actual = list()\n",
        "\n",
        "for word_int in Y_test: \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_2 = word\n",
        "      break\n",
        "  next_in_sequence_actual.append(out_word_2)\n",
        "  \n",
        "print(next_in_sequence_predicted)\n",
        "print(next_in_sequence_actual)\n",
        "\n",
        "# with new sentence to check if it has learnt semantics of vocabulaty\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it was my fault to think that the little boy was poor and unhappy I could not see the sad look the wicked woman used to give me as she handed me my bread unfortunately I was always hungry he emptied his glass while reading the newspaper he laid his napkin\n",
            "\n",
            "With random seed data....\n",
            "it was my fault to think that the little boy was poor and unhappy I could not see the sad look the wicked woman used to give me as she handed me my bread unfortunately I was always hungry he emptied his glass while reading the newspaper he laid his napkin\n",
            "on the pockets of his red trousers and shivering in his sheepskin coat he gave himself up to a sombre thoughts this defeated soldier and looked with a wretched winter sky across in one shudder the roof lighting in the ground floor of the cafeanglais and was a large young\n",
            "With test data....\n",
            "['offends']\n",
            "['offends']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TzkMuyp5UWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model: Model 3\n",
        "# Text: republic.txt\n",
        "\n",
        "in_filename = 'republic_sequences.txt'     # we use the same file\n",
        "model_name = 'model_3_best_republic.h5'\n",
        "tokenizer_name = 'tokenizer_republic.pkl'\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frsgM1tVe9L7",
        "colab_type": "text"
      },
      "source": [
        "# Understanding Individual Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjuYcC5TfBtp",
        "colab_type": "code",
        "outputId": "aa982508-b433-4f42-a5c0-a3e1c463e0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Experiment 1: working of enbedding layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = 856, \n",
        "                    output_dim = 100, \n",
        "                    input_length=50))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = 'rmsprop',\n",
        "              loss = 'mse')\n",
        "\n",
        "# no fit since embedding only maps words to reduce dimensionality fo the vocabulary\n",
        "# creates a dense vector from a sparse vocabulary\n",
        "\n",
        "embedding_layer_output = model.predict(X_data)\n",
        "\n",
        "print(\"Input shape: {0} Input type {1} transformed to \".format(X_data.shape, type(X_data)))\n",
        "print(\"Output shape {0}  Output type {1}\".format(embedding_layer_output.shape, type(embedding_layer_output)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 50, 100)           85600     \n",
            "=================================================================\n",
            "Total params: 85,600\n",
            "Trainable params: 85,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5KUXM_tv68y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_doc = load_doc('glove.6B.100d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPLGt9JkyiVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(lines)\n",
        "lines[:400]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX8ouE3zy7iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in glove_doc:\n",
        "  lines.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfFl-gOd0pdb",
        "colab_type": "code",
        "outputId": "405c61fd-0231-4e92-b873-04bb85a05adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "glove_dir = '/Users/mrinalrawool/Downloads/glove.6B'\n",
        "import os\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e47983d0663d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglove_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/mrinalrawool/Downloads/glove.6B'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mrinalrawool/Downloads/glove.6B/glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXhB3hKZ8i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using Glove 6B\n",
        "\n",
        "\n",
        "# glove_dir = '/Users/mrinalrawool/Downloads/glove.6B'\n",
        "embeddings_index = {}\n",
        "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "glove_doc = load_doc('glove.6B.100d.txt')\n",
        "print(f.shape)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 100\n",
        "max_words = vocab_size # (tokenizer.word_index + 1)\n",
        "\n",
        "# for every word in the tokenizer word index, if the word exists in embedding_index; \n",
        "# load the co-effs in embedding_dim\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6WovOeNK_Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment 2: Undertanding LSTM and using it in a small architecture.\n",
        "\n",
        "# =========================================   Preparing test sets   =========================================\n",
        "\n",
        "X_train = X_data[:1554]\n",
        "Y_train = Y_data[:1554]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[1554:2330]\n",
        "Y_validation = Y_data[1554:2330]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[2330:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "Y_test = next_in_sequence_actual_int[2330:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n",
        "\n",
        "\n",
        "# =========================================  The model   =========================================\n",
        "# Working of LSTM\n",
        "# while calculating the transformations, using None instead of #samples works too\n",
        "\n",
        "# vocab size = 856\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = 856, \n",
        "                    output_dim = 100, \n",
        "                    input_length = 50))\n",
        "# input: (samples = 2338, features = 50)\n",
        "# output: (timesteps = 2338, samples = 50, features = 100)\n",
        "model.add(LSTM(units = 100, \n",
        "               activation = 'tanh', \n",
        "               use_bias = 'False'))\n",
        "# output: (None, 100) \n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output: (None, 100) \n",
        "# following layer should always have # units = dict size so that it matches the output\n",
        "model.add(Dense(856, activation='softmax'))\n",
        "# output: (None, 856) \n",
        "# Shape of labels is: (2338, 856)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#x = X_data, y = Y_data, \n",
        "training_history = model.fit(x = X_data, \n",
        "                             y = Y_data, \n",
        "                             batch_size = 20, \n",
        "                             epochs = 34,\n",
        "                             validation_data=(X_validation, Y_validation))\n",
        "\n",
        "# =========================================  Testing and Evaluation  =========================================\n",
        "\n",
        "plot_acc_loss(training_history)\n",
        "\n",
        "next_in_sequence_int = model.predict_classes(X_test) # note we use predict classes\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "next_in_sequence_predicted = list()\n",
        "next_in_sequence_actual = list()\n",
        "\n",
        "for word_int in next_in_sequence_int:\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_1 = word\n",
        "      break\n",
        "  next_in_sequence_predicted.append(out_word_1)\n",
        "\n",
        "for word_int in Y_test: \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_2 = word\n",
        "      break\n",
        "  next_in_sequence_actual.append(out_word_2)\n",
        "  \n",
        "print(next_in_sequence_predicted)\n",
        "print(next_in_sequence_actual)\n",
        "\n",
        "\n",
        "# Observations\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This was a network with one LSTM layers with # units = output dimensionality of the embedding layer. \n",
        "The model converges fairly quickly (14 sec to 10 sec).\n",
        "Furthermore, the model quickly learnt sequences with validation loss at 90%.\n",
        "The model predicted 7/8 words correctly.\n",
        "\n",
        "Possible experiments include\n",
        "1. Hiding a complete sentence from the model before it is tokenized and feeding it as test data.\n",
        "2. Reducing output dimensionality of embedding layer while keeping # LSTM units constant.\n",
        "3. Using Glove embeddings.\n",
        "4. Repeating the experiments with different larger texts.\n",
        "\n",
        "Epoch 34/34\n",
        "2338/2338 [==============================] - 10s 4ms/step - loss: 0.6610 - acc: 0.8618 - val_loss: 0.5331 - val_acc: 0.9008\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAXAIxKHm1TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment 2: Undertanding LSTM and using it in a small architecture.\n",
        "\n",
        "# =========================================   Preparing test sets   =========================================\n",
        "\n",
        "X_train = X_data[:1554]\n",
        "Y_train = Y_data[:1554]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[1554:2330]\n",
        "Y_validation = Y_data[1554:2330]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[2330:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "Y_test = next_in_sequence_actual_int[2330:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n",
        "\n",
        "\n",
        "# =========================================  The model   =========================================\n",
        "# Working of LSTM\n",
        "# while calculating the transformations, using None instead of #samples works too\n",
        "\n",
        "# vocab size = 856\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = 856, \n",
        "                    output_dim = 100, \n",
        "                    input_length = 50))\n",
        "# input: (samples = 2338, features = 50)\n",
        "# output: (timesteps = 2338, samples = 50, features = 100)\n",
        "model.add(LSTM(20, return_sequences=True))\n",
        "model.add(LSTM(10))\n",
        "# output: (None, 100) \n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output: (None, 100) \n",
        "# following layer should always have # units = dict size so that it matches the output\n",
        "model.add(Dense(856, activation='softmax'))\n",
        "# output: (None, 856) \n",
        "# Shape of labels is: (2338, 856)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "training_history = model.fit(x = X_data, \n",
        "                             y = Y_data, \n",
        "                             batch_size = 20, \n",
        "                             epochs = 34,\n",
        "                             validation_data=(X_validation, Y_validation))\n",
        "\n",
        "# =========================================  Testing and Evaluation  =========================================\n",
        "\n",
        "plot_acc_loss(training_history)\n",
        "\n",
        "next_in_sequence_int = model.predict_classes(X_test) # note we use predict classes\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "next_in_sequence_predicted = list()\n",
        "next_in_sequence_actual = list()\n",
        "\n",
        "for word_int in next_in_sequence_int:\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_1 = word\n",
        "      break\n",
        "  next_in_sequence_predicted.append(out_word_1)\n",
        "\n",
        "for word_int in Y_test: \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_2 = word\n",
        "      break\n",
        "  next_in_sequence_actual.append(out_word_2)\n",
        "  \n",
        "print(next_in_sequence_predicted)\n",
        "print(next_in_sequence_actual)\n",
        "\n",
        "# Observations\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This was a deeper network with two LSTM layers with fewer units. \n",
        "The model took a long time per epoch (23 sec to 19 sec).\n",
        "Furthermore, the model did not learn anything valuable as the training accuracy hoveered around 21%.\n",
        "This led to the predictions being completely incorrect\n",
        "\n",
        "Epoch 34/34\n",
        "2338/2338 [==============================] - 19s 8ms/step - loss: 3.2064 - acc: 0.2126 - val_loss: 3.0521 - val_acc: 0.2487\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wACYd2Bdvc3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment 3: Undertanding LSTM and using it in a small architecture.\n",
        "\n",
        "# =========================================   Preparing test sets   =========================================\n",
        "\n",
        "X_train = X_data[:1554]\n",
        "Y_train = Y_data[:1554]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[1554:2330]\n",
        "Y_validation = Y_data[1554:2330]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[2330:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "Y_test = next_in_sequence_actual_int[2330:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n",
        "\n",
        "\n",
        "# =========================================  The model   =========================================\n",
        "# Working of LSTM\n",
        "# while calculating the transformations, using None instead of #samples works too\n",
        "\n",
        "# vocab size = 856\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = 856, \n",
        "                    output_dim = 50, \n",
        "                    input_length = 50))\n",
        "# input: (samples = 2338, features = 50)\n",
        "# output: (timesteps = 2338, samples = 50, features = 100)\n",
        "model.add(LSTM(units = 100, \n",
        "               activation = 'tanh', \n",
        "               use_bias = 'False'))\n",
        "# output: (None, 100) \n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output: (None, 100) \n",
        "# following layer should always have # units = dict size so that it matches the output\n",
        "model.add(Dense(856, activation='softmax'))\n",
        "# output: (None, 856) \n",
        "# Shape of labels is: (2338, 856)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "training_history = model.fit(x = X_data, \n",
        "                             y = Y_data, \n",
        "                             batch_size = 20, \n",
        "                             epochs = 34,\n",
        "                             validation_data=(X_validation, Y_validation))\n",
        "\n",
        "# =========================================  Testing and Evaluation  =========================================\n",
        "\n",
        "plot_acc_loss(training_history)\n",
        "\n",
        "next_in_sequence_int = model.predict_classes(X_test) # note we use predict classes\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "next_in_sequence_predicted = list()\n",
        "next_in_sequence_actual = list()\n",
        "\n",
        "for word_int in next_in_sequence_int:\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_1 = word\n",
        "      break\n",
        "  next_in_sequence_predicted.append(out_word_1)\n",
        "\n",
        "for word_int in Y_test: \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_2 = word\n",
        "      break\n",
        "  next_in_sequence_actual.append(out_word_2)\n",
        "  \n",
        "print(next_in_sequence_predicted)\n",
        "print(next_in_sequence_actual)\n",
        "\n",
        "\n",
        "# Observations\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This was a network with one LSTM layers with # units > output dimensionality of the embedding layer. \n",
        "The model converges fairly quickly (15 sec to 10 sec).\n",
        "Furthermore, the model quickly learnt sequences with validation loss at 66%.\n",
        "The model predicted all words correctly bu that might just be co-incidence.\n",
        "\n",
        "Epoch 34/34\n",
        "2338/2338 [==============================] - 11s 5ms/step - loss: 1.6142 - acc: 0.5774 - val_loss: 1.4094 - val_acc: 0.6649\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKIdsLoOxvQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment 4: Undertanding LSTM and using it in a small architecture.\n",
        "\n",
        "# =========================================   Preparing test sets   =========================================\n",
        "\n",
        "X_train = X_data[:1554]\n",
        "Y_train = Y_data[:1554]\n",
        "\n",
        "print(\"Training Input shape {0} ; Output shape {1}\".format(X_train.shape, Y_train.shape))\n",
        "\n",
        "X_validation = X_data[1554:2330]\n",
        "Y_validation = Y_data[1554:2330]\n",
        "\n",
        "print(\"Validation Input shape {0} ; Output shape {1}\".format(X_validation.shape, Y_validation.shape))\n",
        "\n",
        "X_test = X_data[2330:]\n",
        "\n",
        "# since we need actual tokenized ints and not categorical ints\n",
        "array_doc_lines_int_seq = array(doc_lines_int_seq)\n",
        "next_in_sequence_actual_int = array_doc_lines_int_seq[:,-1]\n",
        "Y_test = next_in_sequence_actual_int[2330:]\n",
        "\n",
        "print(\"Test Input shape {0} ; Output shape {1}\".format(X_test.shape, Y_test.shape))\n",
        "\n",
        "\n",
        "# =========================================  The model   =========================================\n",
        "# Working of LSTM\n",
        "# while calculating the transformations, using None instead of #samples works too\n",
        "\n",
        "# vocab size = 856\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = 856, \n",
        "                    output_dim = 50, \n",
        "                    input_length = 50))\n",
        "# input: (samples = 2338, features = 50)\n",
        "# output: (timesteps = 2338, samples = 50, features = 100)\n",
        "model.add(LSTM(units = 50, \n",
        "               activation = 'tanh', \n",
        "               use_bias = 'False'))\n",
        "# output: (None, 100) \n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output: (None, 100) \n",
        "# following layer should always have # units = dict size so that it matches the output\n",
        "model.add(Dense(856, activation='softmax'))\n",
        "# output: (None, 856) \n",
        "# Shape of labels is: (2338, 856)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "training_history = model.fit(x = X_data, \n",
        "                             y = Y_data, \n",
        "                             batch_size = 20, \n",
        "                             epochs = 34,\n",
        "                             validation_data=(X_validation, Y_validation))\n",
        "\n",
        "# =========================================  Testing and Evaluation  =========================================\n",
        "\n",
        "plot_acc_loss(training_history)\n",
        "\n",
        "next_in_sequence_int = model.predict_classes(X_test) # note we use predict classes\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "next_in_sequence_predicted = list()\n",
        "next_in_sequence_actual = list()\n",
        "\n",
        "for word_int in next_in_sequence_int:\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_1 = word\n",
        "      break\n",
        "  next_in_sequence_predicted.append(out_word_1)\n",
        "\n",
        "for word_int in Y_test: \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == word_int:\n",
        "      out_word_2 = word\n",
        "      break\n",
        "  next_in_sequence_actual.append(out_word_2)\n",
        "  \n",
        "print(next_in_sequence_predicted)\n",
        "print(next_in_sequence_actual)\n",
        "\n",
        "\n",
        "# Observations\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This was a network with one LSTM layers with # units = output dimensionality of the embedding layer. Both reduced to 50\n",
        "The model converges fairly quickly (15 sec to 10 sec).\n",
        "Furthermore, the model quickly learnt sequences with validation loss at 66%.\n",
        "The model predicted 6/8 words correctly.\n",
        "\n",
        "Epoch 34/34\n",
        "2338/2338 [==============================] - 10s 4ms/step - loss: 1.5056 - acc: 0.6322 - val_loss: 1.3629 - val_acc: 0.6765\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}