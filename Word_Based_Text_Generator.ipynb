{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-Based-Text-Generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JEpB_uu0MBAl",
        "TXx4iof0Q4Wn",
        "tjT8nk3hSWja"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinal-r/Experiments/blob/master/Word_Based_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEpB_uu0MBAl",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jafVWSuxL0Ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7cd54333-7135-4d84-87c0-07d332f6eead"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYMG8FMGMiHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all modules to be used in the project\n",
        "\n",
        "import string\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from pickle import dump\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from random import randint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import floor\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iimZBcsZNa2p",
        "colab_type": "text"
      },
      "source": [
        "# Functions\n",
        "\n",
        "This section contains functions used for data preprocessing, feature engineering, and utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAMkQEaTflSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Load doc into memory\n",
        "# Input: file name\n",
        "# Output: text\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpB5XaG4f4cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: turn a doc into clean tokens\n",
        "# Input: document\n",
        "# Output: a list of tokens that are all in lowercase, have no special characters, split at white spaces\n",
        "\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token ; string-to-replace, string-to-be-replaced-with, string-to-delete\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vngMwI6fhvKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: organize into sequences of tokens\n",
        "# Input: document tokens, input sequence length\n",
        "# Output: document broken down into sequences seperated by WS\n",
        "\n",
        "def create_sequences(tokens, seq_length):\n",
        "  print('Input sequence length is %d'% seq_length)\n",
        "  # length = in_length + out_length\n",
        "  total_length = seq_length+1\n",
        "  #declare a variable to hold the sequences\n",
        "  doc_sequences = list()\n",
        "  for i in range(total_length, len(tokens)):\n",
        "    # list of tokens; list size = total sequence length\n",
        "    curr_line_seq = tokens[i-total_length:i]\n",
        "    # create one string sequence seperated by WS\n",
        "    curr_line = ' '.join(curr_line_seq)\n",
        "    # append it to list of such sequences\n",
        "    doc_sequences.append(curr_line)\n",
        "  return doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM-lk__rjGyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: save a document\n",
        "# Input: document , filename\n",
        "# Output: none\n",
        "\n",
        "def save_doc(doc, doc_filename):\n",
        "\tdata = '\\n'.join(doc)\n",
        "\tfile = open(doc_filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "  \n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqS4_TDjYwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: load a file containing document as sequence\n",
        "# Input: filename\n",
        "# Output: document sequences\n",
        "\n",
        "def load_sequenced_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\tsaved_doc_sequences = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn saved_doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjpPZTpkAEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: convert sequences to integer vectors\n",
        "# Input: list of sequences (text)\n",
        "# Output: list of sequences (int) (list of lists) , vocabulary size\n",
        "# Uses: save_tokenizer_artifacts\n",
        "\n",
        "def text_to_int_tokenize(text_seq, tokenizer_name):\n",
        "  # tokenize\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text_seq)\n",
        "  int_seq = tokenizer.texts_to_sequences(text_seq)\n",
        "\n",
        "  # find the vocab size\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  save_tokenizer_artifacts(tokenizer, tokenizer_name)\n",
        "  return int_seq, vocab_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzJcAw3glWt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Create input and labels\n",
        "# Input: tokenized document sequences (), size of vocabulary\n",
        "# Output: inputs (), Labels ()\n",
        "\n",
        "def create_inputs_and_labels(tokenized_doc_seq, vocab_size):\n",
        "  # convert list to array\n",
        "  array_tokenized_doc_seq = array(tokenized_doc_seq)\n",
        "  inputs = array_tokenized_doc_seq[:,:-1]\n",
        "  labels = array_tokenized_doc_seq[:,-1]\n",
        "  # one hot encode labels; # columns = vocab size\n",
        "  labels = to_categorical(labels, num_classes=vocab_size)\n",
        "  return inputs,labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VfIDGDnvwpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save tokenizer to file\n",
        "# Input: tokenizer(model), name\n",
        "# Output: file saved in your temp workspace\n",
        "\n",
        "def save_tokenizer_artifacts(tokenizer, tokenizer_name):\n",
        "  \n",
        "  # save the tokenizer\n",
        "  dump(tokenizer, open(tokenizer_name, 'wb'))\n",
        "  \n",
        "  files.download(tokenizer_name)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URXAOFL_v9nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save model to file\n",
        "# Input: model, name\n",
        "# Output: file saved in your temp workspace\n",
        "\n",
        "def save_model_artifacts(model_name, model):\n",
        "  # save the model to file\n",
        "  model.save(model_name)\n",
        "  \n",
        "  files.download(model_name)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLjjGsanoy-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Generates the next n words in the sequence using trained model\n",
        "# Input: model, tokenizer, seed text, length of sequence and # words to be predicted\n",
        "# Output: concatenated predicted words\n",
        "\n",
        "def generate_seq(model, tokenizer, input_seq_length, in_text, n_words):\n",
        "\tresult = list()\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tin_text_encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tin_text_encoded = pad_sequences([in_text_encoded], maxlen=input_seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tnext_word_int = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == next_word_int:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM__vml9pPWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: wrapper function for generate_seq\n",
        "# Input: name of the file containing the test sequences, name of the model and the tokenizer\n",
        "# Output: seed text and generated text\n",
        "# Uses: generate_seq()\n",
        "\n",
        "def predict_from_seed_data(in_filename, model_name, tokenizer_name):\n",
        "  \n",
        "  # load cleaned text sequences: list oflists\n",
        "  text_sequence_doc = load_doc(in_filename)\n",
        "  text_sequence_lines = text_sequence_doc.split('\\n')\n",
        "  \n",
        "  # load the model\n",
        "  my_model = load_model(model_name)\n",
        "  \n",
        "  # load the tokenizer\n",
        "  my_tokenizer = load(open(tokenizer_name, 'rb'))\n",
        "  \n",
        "  seed_text = text_sequence_lines[randint(0,len(text_sequence_lines))]\n",
        "  print(seed_text + '\\n')\n",
        "  \n",
        "  input_seq_length = len(text_sequence_lines[0].split()) - 1\n",
        "  generated_text = generate_seq(my_model, my_tokenizer, input_seq_length, seed_text, 50)\n",
        "  \n",
        "  return seed_text, generated_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBcWRVKnUrSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_acc_loss(model):\n",
        "  plt.plot(model.history['acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(model.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train'], loc='upper left')\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJdLR17Q0iC",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXx4iof0Q4Wn",
        "colab_type": "text"
      },
      "source": [
        "# Model 1: Embedding - LSTM - Dense\n",
        "\n",
        "This is one of the models used for benchmarking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cuta8QhROC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector\n",
        "\n",
        "def Embed_LSTM_Dense(vocab_size, input_seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = vocab_size, \n",
        "                      output_dim = 50, \n",
        "                      input_length=input_seq_length))\n",
        "  model.add(LSTM(100, return_sequences=True))\n",
        "  model.add(LSTM(10))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jewSiZjhSNJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_model = Embed_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                               optimizer='adam', \n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_model_history = Embed_LSTM_Dense_model.fit(X_data, \n",
        "                                                            Y_data, \n",
        "                                                            batch_size=128, \n",
        "                                                            epochs=1) \n",
        "\n",
        "# print model\n",
        "print_model(Embed_LSTM_Dense_model, 'Embed_LSTM_Dense_model.png')\n",
        "\n",
        "# save model\n",
        "model_name = 'model_1.h5'\n",
        "\n",
        "save_model_artifacts(model_name, Embed_LSTM_Dense_model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjT8nk3hSWja",
        "colab_type": "text"
      },
      "source": [
        "# Model 2: Embedding - Convolutional - LSTM - Dense\n",
        "\n",
        "Used for benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkMJ1WTMsfdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector, Drop out rate\n",
        "\n",
        "def Embed_Conv_LSTM_Dense(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                             output_dim = 100, \n",
        "                             input_length=input_seq_length))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(64, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    return model\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDNRpQP7yCwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "# Create\n",
        "Embed_Conv_LSTM_Dense_model = Embed_Conv_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_Conv_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_Conv_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                                    optimizer='adam', \n",
        "                                    metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "Embed_Conv_LSTM_Dense_model.fit(X_data, \n",
        "                                Y_data, \n",
        "                                batch_size=128, \n",
        "                                epochs=1) \n",
        "\n",
        "# print model\n",
        "print_model(Embed_Conv_LSTM_Dense_model, 'Embed_Conv_LSTM_Dense_model.png')\n",
        "\n",
        "# save model\n",
        "model_name = 'model_2.h5'\n",
        "\n",
        "save_model_artifacts(model_name, Embed_Conv_LSTM_Dense_model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChHOcsISrBr",
        "colab_type": "text"
      },
      "source": [
        "# Model 3: Embedding - LSTM - Dense (My model)\n",
        "\n",
        "Simple architecture that would help understand the behavior of the model. This model also gives me flexibility to change parameters and understand the relationship of parameters to output.\n",
        "\n",
        "Simple 4 layer model with input sequence = 0.5 of output dimentionality of embedding layer and output dimentionality of embedding layer = # units of LSTM layer that follows.\n",
        "\n",
        "Lets implement callbacks here since we know this model works well with the previous data set. So with a bigger text, we would like to save our model along the way so that we have something to work with of the session crashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV_JVoNXd4pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: None as of now since this model converges well and gives impressive results\n",
        "# number at the end indicates number of layers\n",
        "# Observations: # dims for embed layer = # LSTM units = # Dense units - kept on purpose\n",
        "# might consider experimenting later to see if performance is affected\n",
        "\n",
        "def Embed_LSTM_Dense_4(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                    output_dim = 100, \n",
        "                    input_length = input_seq_length))\n",
        "    # input: (samples = 2338, features = 50)\n",
        "    # output: (timesteps = 2338, samples = 50, features = 100)\n",
        "    # added dropout when it performed bad with validation data\n",
        "    model.add(LSTM(units = 100, \n",
        "                   dropout = 0.5,\n",
        "                   recurrent_dropout = 0.2,\n",
        "                   activation = 'tanh', \n",
        "                   use_bias = 'False'))\n",
        "    # output: (None, 100) \n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    # output: (None, 100) \n",
        "    # following layer should always have # units = dict size so that it matches the output\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # output: (None, 856) \n",
        "    # Shape of labels is: (2338, 856)\n",
        "    return model\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FTVrZaAfk7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4569
        },
        "outputId": "ad619ac7-153e-41cb-8843-99601d7982b6"
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "model_name = 'model_3_pob.h5'\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_4_model = Embed_LSTM_Dense_4(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_4_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_4_model.compile(loss='categorical_crossentropy', \n",
        "                                 optimizer='adam', \n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "# adding a checkpoint| we monitor accuracy so the mode will be maximixed; for loss, its minimized\n",
        "# we can leave it as auto but I'm explicitely mentioning as max\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_savepoint = keras.callbacks.ModelCheckpoint(filepath= model_name, \n",
        "                                                  monitor='acc', \n",
        "                                                  verbose = 0 ,\n",
        "                                                  save_best_only=True ,\n",
        "                                                  save_weights_only=False, \n",
        "                                                  mode='max', \n",
        "                                                  period=1)\n",
        "\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_4_model_history = Embed_LSTM_Dense_4_model.fit(x = X_data, \n",
        "                                                                y = Y_data, \n",
        "                                                                callbacks = [model_savepoint],\n",
        "                                                                batch_size=128, \n",
        "                                                                epochs=100) \n",
        "\n",
        "# plot\n",
        "plot_acc_loss(Embed_LSTM_Dense_4_model_history)\n",
        "\n",
        "files.download(model_name)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 100)           85600     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 856)               86456     \n",
            "=================================================================\n",
            "Total params: 262,556\n",
            "Trainable params: 262,556\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2338/2338 [==============================] - 2s 1ms/step - loss: 6.7308 - acc: 0.0252\n",
            "Epoch 2/100\n",
            "2338/2338 [==============================] - 2s 671us/step - loss: 6.0605 - acc: 0.0466\n",
            "Epoch 3/100\n",
            "2338/2338 [==============================] - 2s 687us/step - loss: 5.8109 - acc: 0.0744\n",
            "Epoch 4/100\n",
            "2338/2338 [==============================] - 2s 696us/step - loss: 5.7827 - acc: 0.0744\n",
            "Epoch 5/100\n",
            "2338/2338 [==============================] - 2s 679us/step - loss: 5.7574 - acc: 0.0744\n",
            "Epoch 6/100\n",
            "2338/2338 [==============================] - 2s 675us/step - loss: 5.7416 - acc: 0.0744\n",
            "Epoch 7/100\n",
            "2338/2338 [==============================] - 2s 689us/step - loss: 5.7094 - acc: 0.0744\n",
            "Epoch 8/100\n",
            "2338/2338 [==============================] - 2s 683us/step - loss: 5.6669 - acc: 0.0744\n",
            "Epoch 9/100\n",
            "2338/2338 [==============================] - 2s 673us/step - loss: 5.6100 - acc: 0.0744\n",
            "Epoch 10/100\n",
            "2338/2338 [==============================] - 2s 692us/step - loss: 5.5490 - acc: 0.0744\n",
            "Epoch 11/100\n",
            "2338/2338 [==============================] - 2s 773us/step - loss: 5.4848 - acc: 0.0744\n",
            "Epoch 12/100\n",
            "2338/2338 [==============================] - 2s 777us/step - loss: 5.4106 - acc: 0.0761\n",
            "Epoch 13/100\n",
            "2338/2338 [==============================] - 2s 778us/step - loss: 5.3257 - acc: 0.0774\n",
            "Epoch 14/100\n",
            "2338/2338 [==============================] - 2s 783us/step - loss: 5.2330 - acc: 0.0817\n",
            "Epoch 15/100\n",
            "2338/2338 [==============================] - 2s 778us/step - loss: 5.1631 - acc: 0.0821\n",
            "Epoch 16/100\n",
            "2338/2338 [==============================] - 2s 712us/step - loss: 5.1050 - acc: 0.0920\n",
            "Epoch 17/100\n",
            "2338/2338 [==============================] - 2s 683us/step - loss: 5.0446 - acc: 0.0945\n",
            "Epoch 18/100\n",
            "2338/2338 [==============================] - 2s 684us/step - loss: 4.9936 - acc: 0.0907\n",
            "Epoch 19/100\n",
            "2338/2338 [==============================] - 2s 693us/step - loss: 4.9423 - acc: 0.0954\n",
            "Epoch 20/100\n",
            "2338/2338 [==============================] - 2s 682us/step - loss: 4.8802 - acc: 0.0992\n",
            "Epoch 21/100\n",
            "2338/2338 [==============================] - 2s 672us/step - loss: 4.8180 - acc: 0.0997\n",
            "Epoch 22/100\n",
            "2338/2338 [==============================] - 2s 701us/step - loss: 4.7684 - acc: 0.0988\n",
            "Epoch 23/100\n",
            "2338/2338 [==============================] - 2s 674us/step - loss: 4.7054 - acc: 0.1052\n",
            "Epoch 24/100\n",
            "2338/2338 [==============================] - 2s 681us/step - loss: 4.6356 - acc: 0.1069\n",
            "Epoch 25/100\n",
            "2338/2338 [==============================] - 2s 680us/step - loss: 4.5796 - acc: 0.1104\n",
            "Epoch 26/100\n",
            "2338/2338 [==============================] - 2s 699us/step - loss: 4.5119 - acc: 0.1022\n",
            "Epoch 27/100\n",
            "2338/2338 [==============================] - 2s 679us/step - loss: 4.4387 - acc: 0.1155\n",
            "Epoch 28/100\n",
            "2338/2338 [==============================] - 2s 673us/step - loss: 4.3529 - acc: 0.1228\n",
            "Epoch 29/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 4.2733 - acc: 0.1296\n",
            "Epoch 30/100\n",
            "2338/2338 [==============================] - 2s 673us/step - loss: 4.2008 - acc: 0.1373\n",
            "Epoch 31/100\n",
            "2338/2338 [==============================] - 2s 675us/step - loss: 4.1546 - acc: 0.1441\n",
            "Epoch 32/100\n",
            "2338/2338 [==============================] - 2s 679us/step - loss: 4.2377 - acc: 0.1416\n",
            "Epoch 33/100\n",
            "2338/2338 [==============================] - 2s 687us/step - loss: 4.1678 - acc: 0.1403\n",
            "Epoch 34/100\n",
            "2338/2338 [==============================] - 2s 675us/step - loss: 4.0492 - acc: 0.1561\n",
            "Epoch 35/100\n",
            "2338/2338 [==============================] - 2s 687us/step - loss: 4.0008 - acc: 0.1540\n",
            "Epoch 36/100\n",
            "2338/2338 [==============================] - 2s 693us/step - loss: 3.8753 - acc: 0.1719\n",
            "Epoch 37/100\n",
            "2338/2338 [==============================] - 2s 676us/step - loss: 3.8280 - acc: 0.1698\n",
            "Epoch 38/100\n",
            "2338/2338 [==============================] - 2s 689us/step - loss: 3.6806 - acc: 0.1963\n",
            "Epoch 39/100\n",
            "2338/2338 [==============================] - 2s 690us/step - loss: 3.5745 - acc: 0.2040\n",
            "Epoch 40/100\n",
            "2338/2338 [==============================] - 2s 680us/step - loss: 3.4978 - acc: 0.2126\n",
            "Epoch 41/100\n",
            "2338/2338 [==============================] - 2s 699us/step - loss: 3.4285 - acc: 0.2233\n",
            "Epoch 42/100\n",
            "2338/2338 [==============================] - 2s 686us/step - loss: 3.3754 - acc: 0.2280\n",
            "Epoch 43/100\n",
            "2338/2338 [==============================] - 2s 682us/step - loss: 3.2916 - acc: 0.2395\n",
            "Epoch 44/100\n",
            "2338/2338 [==============================] - 2s 692us/step - loss: 3.2180 - acc: 0.2357\n",
            "Epoch 45/100\n",
            "2338/2338 [==============================] - 2s 683us/step - loss: 3.1661 - acc: 0.2485\n",
            "Epoch 46/100\n",
            "2338/2338 [==============================] - 2s 693us/step - loss: 3.0908 - acc: 0.2716\n",
            "Epoch 47/100\n",
            "2338/2338 [==============================] - 2s 676us/step - loss: 3.0256 - acc: 0.2827\n",
            "Epoch 48/100\n",
            "2338/2338 [==============================] - 2s 678us/step - loss: 2.9562 - acc: 0.2896\n",
            "Epoch 49/100\n",
            "2338/2338 [==============================] - 2s 721us/step - loss: 2.9096 - acc: 0.2913\n",
            "Epoch 50/100\n",
            "2338/2338 [==============================] - 2s 786us/step - loss: 2.8559 - acc: 0.3118\n",
            "Epoch 51/100\n",
            "2338/2338 [==============================] - 2s 817us/step - loss: 2.7917 - acc: 0.3131\n",
            "Epoch 52/100\n",
            "2338/2338 [==============================] - 2s 819us/step - loss: 2.7463 - acc: 0.3353\n",
            "Epoch 53/100\n",
            "2338/2338 [==============================] - 2s 818us/step - loss: 2.6666 - acc: 0.3494\n",
            "Epoch 54/100\n",
            "2338/2338 [==============================] - 2s 814us/step - loss: 2.6197 - acc: 0.3653\n",
            "Epoch 55/100\n",
            "2338/2338 [==============================] - 2s 801us/step - loss: 2.5660 - acc: 0.3717\n",
            "Epoch 56/100\n",
            "2338/2338 [==============================] - 2s 741us/step - loss: 2.5092 - acc: 0.3794\n",
            "Epoch 57/100\n",
            "2338/2338 [==============================] - 2s 677us/step - loss: 2.4650 - acc: 0.3730\n",
            "Epoch 58/100\n",
            "2338/2338 [==============================] - 2s 746us/step - loss: 2.4153 - acc: 0.3905\n",
            "Epoch 59/100\n",
            "2338/2338 [==============================] - 2s 777us/step - loss: 2.3587 - acc: 0.4042\n",
            "Epoch 60/100\n",
            "2338/2338 [==============================] - 2s 765us/step - loss: 2.3052 - acc: 0.4239\n",
            "Epoch 61/100\n",
            "2338/2338 [==============================] - 2s 775us/step - loss: 2.2424 - acc: 0.4380\n",
            "Epoch 62/100\n",
            "2338/2338 [==============================] - 2s 774us/step - loss: 2.2088 - acc: 0.4525\n",
            "Epoch 63/100\n",
            "2338/2338 [==============================] - 2s 756us/step - loss: 2.1578 - acc: 0.4589\n",
            "Epoch 64/100\n",
            "2338/2338 [==============================] - 2s 677us/step - loss: 2.1183 - acc: 0.4683\n",
            "Epoch 65/100\n",
            "2338/2338 [==============================] - 2s 717us/step - loss: 2.0738 - acc: 0.4726\n",
            "Epoch 66/100\n",
            "2338/2338 [==============================] - 2s 702us/step - loss: 2.0304 - acc: 0.4850\n",
            "Epoch 67/100\n",
            "2338/2338 [==============================] - 2s 676us/step - loss: 1.9989 - acc: 0.5021\n",
            "Epoch 68/100\n",
            "2338/2338 [==============================] - 2s 686us/step - loss: 1.9729 - acc: 0.4927\n",
            "Epoch 69/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 1.9111 - acc: 0.5197\n",
            "Epoch 70/100\n",
            "2338/2338 [==============================] - 2s 692us/step - loss: 1.8798 - acc: 0.5167\n",
            "Epoch 71/100\n",
            "2338/2338 [==============================] - 2s 697us/step - loss: 1.8379 - acc: 0.5308\n",
            "Epoch 72/100\n",
            "2338/2338 [==============================] - 2s 678us/step - loss: 1.7963 - acc: 0.5436\n",
            "Epoch 73/100\n",
            "2338/2338 [==============================] - 2s 686us/step - loss: 1.7738 - acc: 0.5393\n",
            "Epoch 74/100\n",
            "2338/2338 [==============================] - 2s 682us/step - loss: 1.7184 - acc: 0.5624\n",
            "Epoch 75/100\n",
            "2338/2338 [==============================] - 2s 693us/step - loss: 1.6860 - acc: 0.5663\n",
            "Epoch 76/100\n",
            "2338/2338 [==============================] - 2s 699us/step - loss: 1.6533 - acc: 0.5787\n",
            "Epoch 77/100\n",
            "2338/2338 [==============================] - 2s 672us/step - loss: 1.5985 - acc: 0.5915\n",
            "Epoch 78/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 1.5846 - acc: 0.5847\n",
            "Epoch 79/100\n",
            "2338/2338 [==============================] - 2s 680us/step - loss: 1.5476 - acc: 0.5920\n",
            "Epoch 80/100\n",
            "2338/2338 [==============================] - 2s 680us/step - loss: 1.5321 - acc: 0.5950\n",
            "Epoch 81/100\n",
            "2338/2338 [==============================] - 2s 674us/step - loss: 1.4986 - acc: 0.6116\n",
            "Epoch 82/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 1.4418 - acc: 0.6364\n",
            "Epoch 83/100\n",
            "2338/2338 [==============================] - 2s 686us/step - loss: 1.4159 - acc: 0.6390\n",
            "Epoch 84/100\n",
            "2338/2338 [==============================] - 2s 675us/step - loss: 1.3823 - acc: 0.6467\n",
            "Epoch 85/100\n",
            "2338/2338 [==============================] - 2s 676us/step - loss: 1.3820 - acc: 0.6416\n",
            "Epoch 86/100\n",
            "2338/2338 [==============================] - 2s 674us/step - loss: 1.3478 - acc: 0.6506\n",
            "Epoch 87/100\n",
            "2338/2338 [==============================] - 2s 688us/step - loss: 1.2953 - acc: 0.6617\n",
            "Epoch 88/100\n",
            "2338/2338 [==============================] - 2s 698us/step - loss: 1.2803 - acc: 0.6771\n",
            "Epoch 89/100\n",
            "2338/2338 [==============================] - 2s 674us/step - loss: 1.2511 - acc: 0.6737\n",
            "Epoch 90/100\n",
            "2338/2338 [==============================] - 2s 688us/step - loss: 1.2629 - acc: 0.6745\n",
            "Epoch 91/100\n",
            "2338/2338 [==============================] - 2s 686us/step - loss: 1.2082 - acc: 0.6925\n",
            "Epoch 92/100\n",
            "2338/2338 [==============================] - 2s 682us/step - loss: 1.1948 - acc: 0.6938\n",
            "Epoch 93/100\n",
            "2338/2338 [==============================] - 2s 669us/step - loss: 1.1642 - acc: 0.7070\n",
            "Epoch 94/100\n",
            "2338/2338 [==============================] - 2s 691us/step - loss: 1.1358 - acc: 0.7190\n",
            "Epoch 95/100\n",
            "2338/2338 [==============================] - 2s 691us/step - loss: 1.1169 - acc: 0.7160\n",
            "Epoch 96/100\n",
            "2338/2338 [==============================] - 2s 676us/step - loss: 1.0815 - acc: 0.7198\n",
            "Epoch 97/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 1.0748 - acc: 0.7190\n",
            "Epoch 98/100\n",
            "2338/2338 [==============================] - 2s 685us/step - loss: 1.0596 - acc: 0.7151\n",
            "Epoch 99/100\n",
            "2338/2338 [==============================] - 2s 681us/step - loss: 1.0301 - acc: 0.7280\n",
            "Epoch 100/100\n",
            "2338/2338 [==============================] - 2s 680us/step - loss: 0.9978 - acc: 0.7417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERqKRRERKSWQkFERGr9P/iGqHiTH+WtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKNUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-26e4b50275ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mplot_acc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbed_LSTM_Dense_4_model_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDElo7EdTuF6",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLt0ybvCTx0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "46bd05c0-e306-4221-c7c7-7627bd4c6df4"
      },
      "source": [
        "# variables for filenames\n",
        "text_name = 'APieceOfBread'\n",
        "\n",
        "# load document\n",
        "# Variable doc is just a string: <class 'str'>\n",
        "in_filename = \"/content/gdrive/My Drive/DL/\" + text_name + '.txt'\n",
        "doc = load_doc(in_filename)\n",
        "\n",
        "# clean document\n",
        "# Variable token is a list of strings(words): <class 'list'>\n",
        "tokens = clean_doc(doc)\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n",
        "# create sequences from tokens\n",
        "# doc_to_sequences is a list of sequences of 50 words: <class 'list'>\n",
        "doc_to_sequences = list()\n",
        "sequence_length = 50\n",
        "\n",
        "# save sequences to file to be used later for prediction\n",
        "doc_to_sequences = create_sequences(tokens,sequence_length)\n",
        "print('Total Sequences: %d' % len(doc_to_sequences))\n",
        "\n",
        "# doc_lines_seq is a list of sequences of 50 words: <class 'list'>, each elemnt within that list is a string\n",
        "# print(type(doc_lines_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_seq[0])) : <class 'str'>\n",
        "out_filename = text_name + '_sequences.txt'      # change this\n",
        "save_doc(doc_to_sequences, out_filename)\n",
        "\n",
        "# load saved text sequences. do this step if you already have the sequences created earlier\n",
        "in_filename = text_name + '_sequences.txt'      # change this\n",
        "doc_sequences = load_sequenced_doc(in_filename)\n",
        "# load it into a list for processing. Splitting into elemennts by newline\n",
        "doc_lines_seq = doc_sequences.split('\\n')\n",
        "print('Before tokenizing, sample length of input string sequence %d' % len(doc_lines_seq[0]))\n",
        "\n",
        "# Vectorize the sequences using tokenizer. A word to int mapping dictionary will be created\n",
        "# doc_lines_int_seq is a list of lists. Each sequence is a list\n",
        "# print(type(doc_lines_int_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_int_seq[0])) : <class 'list'>\n",
        "tokenizer_name = text_name + '_tokenizer.pkl'      # change this\n",
        "doc_lines_int_seq, sizeof_vocab = text_to_int_tokenize(doc_lines_seq, tokenizer_name)\n",
        "\n",
        "# create data\n",
        "# X_data is 2D tensor (numpy array since data type is same) ; Y_data is 2D tensor\n",
        "# X_data is <class 'numpy.ndarray'> with dimensions # samples, features = words in sequence\n",
        "# Y_data is <class 'numpy.ndarray'> with dimension # samples, features = vocab size - sparse array\n",
        "print('After tokenizing/vectorizing, sample length of input integers sequence %d' % len(doc_lines_int_seq[0]))\n",
        "print('Size of the vocabulary build by the tokenizer %d' % sizeof_vocab)\n",
        "\n",
        "X_data, Y_data = create_inputs_and_labels(doc_lines_int_seq, sizeof_vocab)\n",
        "\n",
        "print(\"Shape of inputs is: {0}\".format(X_data.shape))\n",
        "print(\"Shape of labels is: {0}\".format(Y_data.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 2389\n",
            "Unique Tokens: 855\n",
            "Input sequence length is 50\n",
            "Total Sequences: 2338\n",
            "Before tokenizing, sample length of input string sequence 281\n",
            "After tokenizing/vectorizing, sample length of input integers sequence 51\n",
            "Size of the vocabulary build by the tokenizer 856\n",
            "Shape of inputs is: (2338, 50)\n",
            "Shape of labels is: (2338, 856)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7fBj3bBrhBo",
        "colab_type": "text"
      },
      "source": [
        "# Predicting the next word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th5Wmr9YrlWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model: Model 1\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_1.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI40K3ytrwoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model: Model 2\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_2.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPrifB5Xr0TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model: Model 3\n",
        "\n",
        "in_filename = 'APieceOfBread_sequences.txt'     # we use the same file\n",
        "model_name = 'model_3_best_bread.h5'\n",
        "tokenizer_name = 'tokenizer.pkl'\n",
        "seed_text, generated_text = predict_from_seed_data(in_filename, model_name, tokenizer_name)\n",
        "\n",
        "print(\"With random seed data....\")\n",
        "print(seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}