{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-Based-Text-Generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JEpB_uu0MBAl",
        "TXx4iof0Q4Wn",
        "tjT8nk3hSWja",
        "kChHOcsISrBr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinal-r/Experiments/blob/master/Word_Based_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEpB_uu0MBAl",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jafVWSuxL0Ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7cd54333-7135-4d84-87c0-07d332f6eead"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYMG8FMGMiHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all modules to be used in the project\n",
        "\n",
        "import string\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from pickle import dump\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from random import randint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import floor\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iimZBcsZNa2p",
        "colab_type": "text"
      },
      "source": [
        "# Functions\n",
        "\n",
        "This section contains functions used for data preprocessing, feature engineering, and utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAMkQEaTflSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Load doc into memory\n",
        "# Input: file name\n",
        "# Output: text\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpB5XaG4f4cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: turn a doc into clean tokens\n",
        "# Input: document\n",
        "# Output: a list of tokens that are all in lowercase, have no special characters, split at white spaces\n",
        "\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token ; string-to-replace, string-to-be-replaced-with, string-to-delete\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vngMwI6fhvKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: organize into sequences of tokens\n",
        "# Input: document tokens, input sequence length\n",
        "# Output: document broken down into sequences seperated by WS\n",
        "\n",
        "def create_sequences(tokens, seq_length):\n",
        "  print('Input sequence length is %d'% seq_length)\n",
        "  # length = in_length + out_length\n",
        "  total_length = seq_length+1\n",
        "  #declare a variable to hold the sequences\n",
        "  doc_sequences = list()\n",
        "  for i in range(total_length, len(tokens)):\n",
        "    # list of tokens; list size = total sequence length\n",
        "    curr_line_seq = tokens[i-total_length:i]\n",
        "    # create one string sequence seperated by WS\n",
        "    curr_line = ' '.join(curr_line_seq)\n",
        "    # append it to list of such sequences\n",
        "    doc_sequences.append(curr_line)\n",
        "  return doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM-lk__rjGyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: save a document\n",
        "# Input: document , filename\n",
        "# Output: none\n",
        "\n",
        "def save_doc(doc, doc_filename):\n",
        "\tdata = '\\n'.join(doc)\n",
        "\tfile = open(doc_filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "  \n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqS4_TDjYwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: load a file containing document as sequence\n",
        "# Input: filename\n",
        "# Output: document sequences\n",
        "\n",
        "def load_sequenced_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\tsaved_doc_sequences = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn saved_doc_sequences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjpPZTpkAEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: convert sequences to integer vectors\n",
        "# Input: list of sequences (text)\n",
        "# Output: list of sequences (int) (list of lists) , vocabulary size\n",
        "# Uses: save_tokenizer_artifacts\n",
        "\n",
        "def text_to_int_tokenize(text_seq, tokenizer_name):\n",
        "  # tokenize\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text_seq)\n",
        "  int_seq = tokenizer.texts_to_sequences(text_seq)\n",
        "\n",
        "  # find the vocab size\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  save_tokenizer_artifacts(tokenizer, tokenizer_name)\n",
        "  return int_seq, vocab_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzJcAw3glWt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Create input and labels\n",
        "# Input: tokenized document sequences (), size of vocabulary\n",
        "# Output: inputs (), Labels ()\n",
        "\n",
        "def create_inputs_and_labels(tokenized_doc_seq, vocab_size):\n",
        "  # convert list to array\n",
        "  array_tokenized_doc_seq = array(tokenized_doc_seq)\n",
        "  inputs = array_tokenized_doc_seq[:,:-1]\n",
        "  labels = array_tokenized_doc_seq[:,-1]\n",
        "  # one hot encode labels; # columns = vocab size\n",
        "  labels = to_categorical(labels, num_classes=vocab_size)\n",
        "  return inputs,labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VfIDGDnvwpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save tokenizer to file\n",
        "# Input: tokenizer(model), name\n",
        "# Output: file saved in your temp workspace\n",
        "\n",
        "def save_tokenizer_artifacts(tokenizer, tokenizer_name):\n",
        "  \n",
        "  # save the tokenizer\n",
        "  dump(tokenizer, open(tokenizer_name, 'wb'))\n",
        "  \n",
        "  files.download(tokenizer_name)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URXAOFL_v9nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Save model to file\n",
        "# Input: model, name\n",
        "# Output: file saved in your temp workspace\n",
        "\n",
        "def save_model_artifacts(model_name, model):\n",
        "  # save the model to file\n",
        "  model.save(model_name)\n",
        "  \n",
        "  files.download(model_name)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLjjGsanoy-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: Generates the next n words in the sequence using trained model\n",
        "# Input: model, tokenizer, seed text, length of sequence and # words to be predicted\n",
        "# Output: concatenated predicted words\n",
        "\n",
        "def generate_seq(model, tokenizer, input_seq_length, in_text, n_words):\n",
        "\tresult = list()\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tin_text_encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tin_text_encoded = pad_sequences([in_text_encoded], maxlen=input_seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tnext_word_int = model.predict_classes(in_text_encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == next_word_int:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM__vml9pPWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Purpose: wrapper function for generate_seq\n",
        "# Input: name of the file containing the test sequences, name of the model and the tokenizer\n",
        "# Output: seed text and generated text\n",
        "# Uses: generate_seq()\n",
        "\n",
        "def predict_from_seed_data(in_filename, model_name, tokenizer_name):\n",
        "  \n",
        "  # load cleaned text sequences: list oflists\n",
        "  text_sequence_doc = load_doc(in_filename)\n",
        "  text_sequence_lines = text_sequence_doc.split('\\n')\n",
        "  \n",
        "  # load the model\n",
        "  my_model = load_model(model_name)\n",
        "  \n",
        "  # load the tokenizer\n",
        "  my_tokenizer = load(open(tokenizer_name, 'rb'))\n",
        "  \n",
        "  seed_text = text_sequence_lines[randint(0,len(text_sequence_lines))]\n",
        "  print(seed_text + '\\n')\n",
        "  \n",
        "  input_seq_length = len(text_sequence_lines[0].split()) - 1\n",
        "  generated_text = generate_seq(my_model, my_tokenizer, input_seq_length, seed_text, 50)\n",
        "  \n",
        "  return seed_text, generated_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBcWRVKnUrSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_acc_loss(model):\n",
        "  plt.plot(model.history['acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(model.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train'], loc='upper left')\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJdLR17Q0iC",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXx4iof0Q4Wn",
        "colab_type": "text"
      },
      "source": [
        "# Model 1: Embedding - LSTM - Dense\n",
        "\n",
        "This is one of the models used for benchmarking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cuta8QhROC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector\n",
        "\n",
        "def Embed_LSTM_Dense(vocab_size, input_seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = vocab_size, \n",
        "                      output_dim = 50, \n",
        "                      input_length=input_seq_length))\n",
        "  model.add(LSTM(100, return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jewSiZjhSNJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "model_name = 'model_1_pob.h5'\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_model = Embed_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                               optimizer='adam', \n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "# adding a checkpoint| we monitor accuracy so the mode will be maximixed; for loss, its minimized\n",
        "# we can leave it as auto but I'm explicitely mentioning as max\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_savepoint = keras.callbacks.ModelCheckpoint(filepath= model_name, \n",
        "                                                  monitor='acc', \n",
        "                                                  verbose = 0 ,\n",
        "                                                  save_best_only=True ,\n",
        "                                                  save_weights_only=False, \n",
        "                                                  mode='max', \n",
        "                                                  period=1)\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_model_history = Embed_LSTM_Dense_model.fit(X_data, \n",
        "                                                            Y_data, \n",
        "                                                            callbacks = [model_savepoint],\n",
        "                                                            batch_size=128, \n",
        "                                                            epochs=100) \n",
        "\n",
        "# plot\n",
        "plot_acc_loss(Embed_LSTM_Dense_model_history)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjT8nk3hSWja",
        "colab_type": "text"
      },
      "source": [
        "# Model 2: Embedding - Convolutional - LSTM - Dense\n",
        "\n",
        "Used for benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkMJ1WTMsfdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: Output dim of embedding layer that determines the compactness of the embedding vector, Drop out rate\n",
        "\n",
        "def Embed_Conv_LSTM_Dense(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                             output_dim = 100, \n",
        "                             input_length=input_seq_length))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(64, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    return model\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDNRpQP7yCwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "model_name = 'model_2_pob.h5'\n",
        "\n",
        "# Create\n",
        "Embed_Conv_LSTM_Dense_model = Embed_Conv_LSTM_Dense(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_Conv_LSTM_Dense_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_Conv_LSTM_Dense_model.compile(loss='categorical_crossentropy', \n",
        "                                    optimizer='adam', \n",
        "                                    metrics=['accuracy'])\n",
        "\n",
        "# adding a checkpoint| we monitor accuracy so the mode will be maximixed; for loss, its minimized\n",
        "# we can leave it as auto but I'm explicitely mentioning as max\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_savepoint = keras.callbacks.ModelCheckpoint(filepath= model_name, \n",
        "                                                  monitor='acc', \n",
        "                                                  verbose = 0 ,\n",
        "                                                  save_best_only=True ,\n",
        "                                                  save_weights_only=False, \n",
        "                                                  mode='max', \n",
        "                                                  period=1)\n",
        "\n",
        "# fit model\n",
        "Embed_Conv_LSTM_Dense_model_history = Embed_Conv_LSTM_Dense_model.fit(X_data, \n",
        "                                                                      Y_data, \n",
        "                                                                      callbacks = [model_savepoint],\n",
        "                                                                      batch_size=128, \n",
        "                                                                      epochs=100) \n",
        "\n",
        "# plot\n",
        "plot_acc_loss(Embed_Conv_LSTM_Dense_model_history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChHOcsISrBr",
        "colab_type": "text"
      },
      "source": [
        "# Model 3: Embedding - LSTM - Dense (My model)\n",
        "\n",
        "Simple architecture that would help understand the behavior of the model. This model also gives me flexibility to change parameters and understand the relationship of parameters to output.\n",
        "\n",
        "Simple 4 layer model with input sequence = 0.5 of output dimentionality of embedding layer and output dimentionality of embedding layer = # units of LSTM layer that follows.\n",
        "\n",
        "Lets implement callbacks here since we know this model works well with the previous data set. So with a bigger text, we would like to save our model along the way so that we have something to work with of the session crashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV_JVoNXd4pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies on inputs: Vocabulary size(from tokenizer), input sequence length\n",
        "# tweakable factors: None as of now since this model converges well and gives impressive results\n",
        "# number at the end indicates number of layers\n",
        "# Observations: # dims for embed layer = # LSTM units = # Dense units - kept on purpose\n",
        "# might consider experimenting later to see if performance is affected\n",
        "\n",
        "def Embed_LSTM_Dense_4(vocab_size, input_seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size, \n",
        "                    output_dim = 100, \n",
        "                    input_length = input_seq_length))\n",
        "    # input: (samples = 2338, features = 50)\n",
        "    # output: (timesteps = 2338, samples = 50, features = 100)\n",
        "    # added dropout when it performed bad with validation data\n",
        "    model.add(LSTM(units = 100, \n",
        "                   dropout = 0.5,\n",
        "                   recurrent_dropout = 0.2,\n",
        "                   activation = 'tanh', \n",
        "                   use_bias = 'False'))\n",
        "    # output: (None, 100) \n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    # output: (None, 100) \n",
        "    # following layer should always have # units = dict size so that it matches the output\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # output: (None, 856) \n",
        "    # Shape of labels is: (2338, 856)\n",
        "    return model\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FTVrZaAfk7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this after all the data is prepped\n",
        "\n",
        "model_name = 'model_3_pob.h5'\n",
        "\n",
        "# Create\n",
        "Embed_LSTM_Dense_4_model = Embed_LSTM_Dense_4(sizeof_vocab, X_data.shape[1])\n",
        "\n",
        "# print summary\n",
        "print(Embed_LSTM_Dense_4_model.summary())\n",
        "\n",
        "# compile model\n",
        "Embed_LSTM_Dense_4_model.compile(loss='categorical_crossentropy', \n",
        "                                 optimizer='adam', \n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "# adding a checkpoint| we monitor accuracy so the mode will be maximixed; for loss, its minimized\n",
        "# we can leave it as auto but I'm explicitely mentioning as max\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_savepoint = keras.callbacks.ModelCheckpoint(filepath= model_name, \n",
        "                                                  monitor='acc', \n",
        "                                                  verbose = 0 ,\n",
        "                                                  save_best_only=True ,\n",
        "                                                  save_weights_only=False, \n",
        "                                                  mode='max', \n",
        "                                                  period=1)\n",
        "\n",
        "\n",
        "# fit model\n",
        "Embed_LSTM_Dense_4_model_history = Embed_LSTM_Dense_4_model.fit(x = X_data, \n",
        "                                                                y = Y_data, \n",
        "                                                                callbacks = [model_savepoint],\n",
        "                                                                batch_size=128, \n",
        "                                                                epochs=100) \n",
        "\n",
        "# plot\n",
        "plot_acc_loss(Embed_LSTM_Dense_4_model_history)\n",
        "\n",
        "files.download(model_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDElo7EdTuF6",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLt0ybvCTx0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "46bd05c0-e306-4221-c7c7-7627bd4c6df4"
      },
      "source": [
        "# variables for filenames\n",
        "text_name = 'APieceOfBread'\n",
        "\n",
        "# load document\n",
        "# Variable doc is just a string: <class 'str'>\n",
        "in_filename = \"/content/gdrive/My Drive/DL/\" + text_name + '.txt'\n",
        "doc = load_doc(in_filename)\n",
        "\n",
        "# clean document\n",
        "# Variable token is a list of strings(words): <class 'list'>\n",
        "tokens = clean_doc(doc)\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n",
        "# create sequences from tokens\n",
        "# doc_to_sequences is a list of sequences of 50 words: <class 'list'>\n",
        "doc_to_sequences = list()\n",
        "sequence_length = 50\n",
        "\n",
        "# save sequences to file to be used later for prediction\n",
        "doc_to_sequences = create_sequences(tokens,sequence_length)\n",
        "print('Total Sequences: %d' % len(doc_to_sequences))\n",
        "\n",
        "# doc_lines_seq is a list of sequences of 50 words: <class 'list'>, each elemnt within that list is a string\n",
        "# print(type(doc_lines_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_seq[0])) : <class 'str'>\n",
        "out_filename = text_name + '_sequences.txt'      # change this\n",
        "save_doc(doc_to_sequences, out_filename)\n",
        "\n",
        "# load saved text sequences. do this step if you already have the sequences created earlier\n",
        "in_filename = text_name + '_sequences.txt'      # change this\n",
        "doc_sequences = load_sequenced_doc(in_filename)\n",
        "# load it into a list for processing. Splitting into elemennts by newline\n",
        "doc_lines_seq = doc_sequences.split('\\n')\n",
        "print('Before tokenizing, sample length of input string sequence %d' % len(doc_lines_seq[0]))\n",
        "\n",
        "# Vectorize the sequences using tokenizer. A word to int mapping dictionary will be created\n",
        "# doc_lines_int_seq is a list of lists. Each sequence is a list\n",
        "# print(type(doc_lines_int_seq)) : <class 'list'>\n",
        "# print(type(doc_lines_int_seq[0])) : <class 'list'>\n",
        "tokenizer_name = text_name + '_tokenizer.pkl'      # change this\n",
        "doc_lines_int_seq, sizeof_vocab = text_to_int_tokenize(doc_lines_seq, tokenizer_name)\n",
        "\n",
        "# create data\n",
        "# X_data is 2D tensor (numpy array since data type is same) ; Y_data is 2D tensor\n",
        "# X_data is <class 'numpy.ndarray'> with dimensions # samples, features = words in sequence\n",
        "# Y_data is <class 'numpy.ndarray'> with dimension # samples, features = vocab size - sparse array\n",
        "print('After tokenizing/vectorizing, sample length of input integers sequence %d' % len(doc_lines_int_seq[0]))\n",
        "print('Size of the vocabulary build by the tokenizer %d' % sizeof_vocab)\n",
        "\n",
        "X_data, Y_data = create_inputs_and_labels(doc_lines_int_seq, sizeof_vocab)\n",
        "\n",
        "print(\"Shape of inputs is: {0}\".format(X_data.shape))\n",
        "print(\"Shape of labels is: {0}\".format(Y_data.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 2389\n",
            "Unique Tokens: 855\n",
            "Input sequence length is 50\n",
            "Total Sequences: 2338\n",
            "Before tokenizing, sample length of input string sequence 281\n",
            "After tokenizing/vectorizing, sample length of input integers sequence 51\n",
            "Size of the vocabulary build by the tokenizer 856\n",
            "Shape of inputs is: (2338, 50)\n",
            "Shape of labels is: (2338, 856)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7fBj3bBrhBo",
        "colab_type": "text"
      },
      "source": [
        "# Predicting the next word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVOa4c7Z72Y1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7fbdaa8a-2989-4697-d757-09776af29494"
      },
      "source": [
        "# common testing seed\n",
        "\n",
        "seed_text = doc_lines_seq[randint(0,len(doc_lines_seq))]\n",
        "print(seed_text + '\\n')\n",
        "input_seq_length = len(doc_lines_seq[0].split()) - 1\n",
        "print(\"Input length sequence is {}\".format(input_seq_length))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at the ends which the children would take out of their baskets and throw on the sidewalks as they came from school i used to try to prowl around there when i went on errands at last my time was ended at this trade by which no man can support himself\n",
            "\n",
            "Input length sequence is 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdvV_qrB8NE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ce273292-22a9-466c-9751-c00b3a00f38c"
      },
      "source": [
        "tokenizer_name = 'APieceOfBread_tokenizer.pkl'\n",
        "model1 = 'model_1_pob.h5'\n",
        "model2 = 'model_2_pob.h5'\n",
        "model3 = 'model_3_pob.h5'\n",
        "predict_words = 50\n",
        "my_tokenizer = load(open(tokenizer_name, 'rb'))\n",
        "\n",
        "# Model 1\n",
        "print(\"Testing Model 1\")\n",
        "# load the model 1\n",
        "my_model1 = load_model(model1)\n",
        "generated_text_1 = generate_seq(my_model1, my_tokenizer, input_seq_length, seed_text, predict_words)\n",
        "print(generated_text_1 + '\\n')\n",
        "\n",
        "# Model 2\n",
        "print(\"Testing Model 2\")\n",
        "my_model2 = load_model(model2)\n",
        "generated_text_2 = generate_seq(my_model2, my_tokenizer, input_seq_length, seed_text, predict_words)\n",
        "print(generated_text_2 + '\\n')\n",
        "\n",
        "# Model 3\n",
        "print(\"Testing Model 3\")\n",
        "# load the model 1\n",
        "my_model3 = load_model(model3)\n",
        "generated_text_3 = generate_seq(my_model3, my_tokenizer, input_seq_length, seed_text, predict_words)\n",
        "print(generated_text_3 + '\\n')\n",
        "  \n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Model 1\n",
            "hardimont are not hungry here is the soldier in his soldier at his few let fellow he gave he are just much his forehead it when you soil i have not not although the walking under hardimont crazy and when i was always not just it is is not never\n",
            "\n",
            "Testing Model 2\n",
            "did i not we to things i are you astonished me there i i it the duty with i he is he to he he you you you to to to to to not your me me i he he but to to to to to to to to me\n",
            "\n",
            "Testing Model 3\n",
            "well i do not laugh my friend it offends three years and duty tonight jeanvictor went on their frequent watches and alarms my forehead at the commissary officer all my ration of hunger the time i am just as well i have always remembered the soldier i am just out\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}