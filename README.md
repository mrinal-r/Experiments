# Experiments
This repository is created to host deep learning models

Author: Mrinal Rawool

# Word Based Text Generation with LSTM

Link to the code: https://github.com/mrinal-r/Experiments/blob/master/Word_Based_Text_Generator.ipynb

**Dataset Acquisition and Feature Engineering**

Getting sample texts for this project was fairly straightforward. I used texts from Project Gutenberg (https://www.gutenberg.org/). Project Gutenberg is an organization that provides free literary work by noted authors from across the world. However, this raw text data needs to be preprocessed in order to be consumed by the deep learning model. The way in which the data is processed depends on the nature of our experiment. There a many flavors of text generation/ prediction models like character based, word based, sentence based to name a few. The choice of granualirity dictates what one may deem as important while training. For example, for a word based model, we would prefer getting rid of all kinds of punctuations. However, we might want to retain some punctuations in a sentence based prediction model. Differences also arise in the way in which sequences are generated.

**Data augumentation:**

Planning to use embedding layer trained on Glove dataset (https://nlp.stanford.edu/projects/glove/). The Glove dataset is a carefully curated datset of words and their relationships. I'm hoping that the word prediction would get better and the predicted sentences will be more coherent after using the embedding layer weights trained on Glove dataset.

**Visualization**

Visualization for text generation can be best done by giving a sneak peek at the kind of transformation needed in order to get the data ready to be consumed by the model.

1. Sample text after loading the file:

A PIECE OF BREAD

BY FRANCOIS COPPEE


The young Duc de Hardimont happened to be at Aix in Savoy, whose waters he
hoped would benefit his famous mare, Perichole, who had become wind-broken
since the c

2. After ccleaning and reating string tokens

['a', 'piece', 'of', 'bread', 'by', 'francois', 'coppee', 'the', 'young', 'duc', 'de', 'hardimont', 'happened', 'to', 'be', 'at', 'aix', 'in', 'savoy', 'whose', 'waters', 'he', 'hoped', 'would', 'benefit', 'his', 'famous', 'mare', 'perichole', 'who', 'had', 'become', 'windbroken', 'since', 'the', 'cold']

3. Creating text sequences 

['a piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over', 
'piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the', 
'of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning', 
'bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper', 
'by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper when']

Observe that the next sequences are generated by sliding over the previous sequence by one word.

4. Vectorizing the sequences

Before tokenizing, sample length of input string sequence 281
['a piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over', 'piece of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the', 'of bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning', 'bread by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper', 'by francois coppee the young duc de hardimont happened to be at aix in savoy whose waters he hoped would benefit his famous mare perichole who had become windbroken since the cold she had caught at the last derby and was finishing his breakfast while glancing over the morning paper when']

After tokenizing/vectorizing, sample length of input integers sequence 51
[[4, 115, 3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59], [115, 3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1], [3, 29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156], [29, 21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156, 270], [21, 853, 852, 1, 38, 114, 28, 33, 848, 7, 51, 17, 847, 9, 846, 155, 845, 11, 844, 32, 842, 6, 841, 840, 839, 58, 14, 266, 836, 835, 1, 834, 79, 14, 833, 17, 1, 109, 831, 2, 8, 829, 6, 828, 826, 825, 59, 1, 156, 270, 35]]

Finally, we have the input in the form that can be ingested by the model. 

**Training**

Since we have sequential data, we do not randomize. Also, my online research showed that more often than not, the entire data set is used for training. 

In the above section, I demonstrated how raw data is vectorized. We have a sequence of 51 words represented as integers. For this project, I have set the input sequence length as 50. This means that the first 50 words (integers) will be X, and the 51st word (integer) will be the Y.

Furthermore, we regard textual data as categorical data. Hence Y will be one hot encoded with the # columns = vocabulary size.

**Testing**

For testing, we can use 
1. One of the sequence as seed text to generate the next n words.
2. A new seed text created using words from the dataset


**Architecture Design**

My research was aimed at trying to find a model of decent size that would give me significant results with the resources I have. Generally, LSTMs tend to be costly due to their cell size. They can also prove difficult to debug or to understand the working of the model. The model I have currently is a sequential model having the following layers:

**Embedding Layer:** 
This layer learns the relationship between words. This layer transforms input vocabulary into a word representation in the output dimension space.

**LSTM Layer:** 
This layer uses the output of the embedding layer and the training sequences to learn the nature of the incoming data. Ideally, this layer should help capture the text generation style of the source.

**Dense Layer:** 
This model uses two dense layers which is the observed norm for text generation models. The first dense layer helps flatten the LSTM layet output. The second dense layer transforms it such that it would match the dimensions of the labels.


Please refer to the code to understand the transformation clearly. I have tried to document all the data transformations for the sake of clarity.


Few observations I made while playing around with my model:
1. The model works better when the # output dimensions = 2 * (Length of input sequence)

2. Without using dropout, the model overfits. I have used both dropout and recurrent dropout and I have observed that the model was able to generalize better. Furthermore, the learning rate also slowed down a notch.

## Model Evaluation

I used two models that were published online to benchmark my model. I trained all three models on the same dataset, with the same batchsize and epochs.

All the models were of comparable sizes:

Model 3: Trainable params: 262,556 (mine)

Model 1: Trainable params: 280,156 (benchmark # 1)

Model 2: Trainable params: 232,370 (benchmark # 2)

**Model Performance**

1. Time:
All three models trained fairly quickly. For the same training data, 100 epochs and batch size 128

Model 1 took an average of 3 sec per epoch

Model 2 took less than a second per epoch

**Model 3** took an average of 2 sec per epoch
Model 2 uses Maxpooling which reduces the dimensions and is a fairly fast operation which would have contributed to its computational efficiency.

2. Accuracy:

Model 1:

Epoch 100/100

2338/2338 [==============================] - 3s 1ms/step - loss: 2.5191 - acc: 0.3353


Model 2:

Epoch 100/100

2338/2338 [==============================] - 0s 183us/step - loss: 2.1793 - acc: 0.4299


**Model 3:**

Epoch 100/100

2338/2338 [==============================] - 2s 680us/step - loss: 0.9978 - acc: 0.7417


*It is pretty evident that Model 3 has the best training accuracy. Below are the accuracy and loss plots for the three models.*


Model 1 Accuracy: 

![alt text](https://github.com/mrinal-r/Experiments/blob/master/metrics-images/model1_acc.png "Model 1 Accuracy")


Model 2 Accuracy: 

![alt text](https://github.com/mrinal-r/Experiments/blob/master/metrics-images/model2_acc.png "Model 2 Accuracy")


Model 3 Accuracy: 

![alt text](https://github.com/mrinal-r/Experiments/blob/master/metrics-images/model3_acc.png "Model 3 Accuracy")


# Test Results

For testing, I fed the same seed data to all the three models to generate the next 50 words in sequence. Below are the results:

Seed Text:

at the ends which the children would take out of their baskets and throw on the sidewalks as they came from school i used to try to prowl around there when i went on errands at last my time was ended at this trade by which no man can support himself

Input length sequence is 50

Testing Model 1

hardimont are not hungry here is the soldier in his soldier at his few let fellow he gave he are just much his forehead it when you soil i have not not although the walking under hardimont crazy and when i was always not just it is is not never


Testing Model 2

did i not we to things i are you astonished me there i i it the duty with i he is he to he he you you you to to to to to not your me me i he he but to to to to to to to to me


Testing Model 3

well i do not laugh my friend it offends three years and duty tonight jeanvictor went on their frequent watches and alarms my forehead at the commissary officer all my ration of hunger the time i am just as well i have always remembered the soldier i am just out


**Observations**

Staying true to the training accuracy, output of model 2 does not make any sense. It has few meaningful phrases like *'are you astonished'* but most of it is just word repreated several times.

Model 1 performs better than model 2 in that it doesn't keep repeating words. The model demostrates that it is trying to learn and can spit out different words. However, the words don't make much sense.

My model:
In my opinion, model 3 performs slightly better than the other two. It has a couple of phrases that make sense like *well i do not laugh my friend it offends* and *jeanvictor went on their frequent watches* or *i am just as well i have always remembered*. However, there is still scope for improvement.
